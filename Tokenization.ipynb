{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wm3T6JYOhvg",
    "outputId": "7e624d8c-f5aa-4acc-f195-c6f7a8ed4940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\n",
      "length: 533\n",
      "---\n",
      "length: 616\n"
     ]
    }
   ],
   "source": [
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print('---')\n",
    "#print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XPEjcKJ3Ohvh"
   },
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # we iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1 # default to 0\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "# print(stats)\n",
    "# print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQ9y2WP5Ohvh",
    "outputId": "d787d1c6-2eaf-4880-896f-7ffad150efa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUV5pr3iOhvi",
    "outputId": "6064d636-7ada-44ec-c89d-5142cbf5af40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 199, 9, 1]\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 596\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "  # In the list of ints (ids), replace all consecutive occurences of pair with new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 199))\n",
    "\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZpJtPOc4Ohvi"
   },
   "outputs": [],
   "source": [
    "# making the training text longer to have more representative token statistics\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"\"\"In this post we will describe and demystify the relevant artifacts in the paper “Attention is all you need” (Vaswani, Ashish & Shazeer, Noam & Parmar, Niki & Uszkoreit, Jakob & Jones, Llion & Gomez, Aidan & Kaiser, Lukasz & Polosukhin, Illia. (2017))[1]. This paper was a great advance in the use of the attention mechanism, being the main improvement for a model called Transformer. The most famous current models that are emerging in NLP tasks consist of dozens of transformers or some of their variants, for example, GPT-2 or BERT.\n",
    "\n",
    "We will describe the components of this model, analyze their operation and build a simple model that we will apply to a small-scale NMT problem (Neural Machine Translation). To read more about the problem that we will address and to know how the basic attention mechanism works, I recommend you to read my previous post “A Guide on the Encoder-Decoder Model and the Attention Mechanism”.\n",
    "\n",
    "Why we need Transformer\n",
    "In sequence-to-sequence problems such as the neural machine translation, the initial proposals were based on the use of RNNs in an encoder-decoder architecture. These architectures have a great limitation when working with long sequences, their ability to retain information from the first elements was lost when new elements were incorporated into the sequence. In the encoder, the hidden state in every step is associated with a certain word in the input sentence, usually one of the most recent. Therefore, if the decoder only accesses the last hidden state of the decoder, it will lose relevant information about the first elements of the sequence. Then to deal with this limitation, a new concept were introduced the attention mechanism.\n",
    "\n",
    "Instead of paying attention to the last state of the encoder as is usually done with RNNs, in each step of the decoder we look at all the states of the encoder, being able to access information about all the elements of the input sequence. This is what attention does, it extracts information from the whole sequence, a weighted sum of all the past encoder states. This allows the decoder to assign greater weight or importance to a certain element of the input for each element of the output. Learning in every step to focus in the right element of the input to predict the next output element.\n",
    "\n",
    "But this approach continues to have an important limitation, each sequence must be treated one element at a time. Both the encoder and the decoder have to wait till the completion of t-1 steps to process thet-th step. So when dealing with huge corpus it is very time consuming and computationally inefficient.\n",
    "\n",
    "What is the Transformer?\n",
    "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization … the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
    "\n",
    "“Attention is all you need” paper [1]\n",
    "\n",
    "The Transformer model extract features for each word using a self-attention mechanism to figure out how important all the other words in the sentence are w.r.t. to the aforementioned word. And no recurrent units are used to obtain this features, they are just weighted sums and activations, so they can be very parallelizable and efficient.\n",
    "\n",
    "But we will dive deeper into its architecture (next figure) to understand what all this pieces do [1].\n",
    "\n",
    "\n",
    "From “Attention is all you need” paper by Vaswani, et al., 2017 [1]\n",
    "We can observe there is an encoder model on the left side and the decoder on the right one. Both contains a core block of “an attention and a feed-forward network” repeated N times. But first we need to explore a core concept in depth: the self-attention mechanism.\n",
    "\n",
    "Self-Attention: the fundamental operation\n",
    "Self-attention is a sequence-to-sequence operation: a sequence of vectors goes in, and a sequence of vectors comes out. Let’s call the input vectors x1, x2,…, xt and the corresponding output vectors y1, y2,…, yt. The vectors all have dimension k. To produce output vector yi, the self attention operation simply takes a weighted average over all the input vectors, the simplest option is the dot product.\n",
    "\n",
    "Transformers from scratch by Peter Bloem [2]\n",
    "\n",
    "In the self-attention mechanism of our model we need to introduce three elements: Queries, Values and Keys\n",
    "\n",
    "The Query, The Value and The Key\n",
    "Every input vector is used in three different ways in the self-attention mechanism: the Query, the Key and the Value. In every role, it is compared to the other vectors to get its own output yi(Query), to get the j-th output yj(Key) and to compute each output vector once the weights have been established (Value).\n",
    "\n",
    "To obtain this roles, we need three weight matrices of dimensions k x k and compute three linear transformation for each xi:\n",
    "\n",
    "\n",
    "“Transformers from scratch” by Peter Bloem [2]\n",
    "These three matrices are usually known as K, Q and V, three learnable weight layers that are applied to the same encoded input. Consequently, as each of these three matrices come from the same input, we can apply the attention mechanism of the input vector with itself, a “self-attention”.\n",
    "\n",
    "The Scaled Dot-Product Attention\n",
    "The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot product of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.\n",
    "\n",
    "“Attention is all you need” paper [1]\n",
    "\n",
    "Then we use the Q, K and V matrices to calculate the attention scores. The scores measure how much focus to place on other places or words of the input sequence w.r.t a word at a certain position. That is, the dot product of the query vector with the key vector of the respective word we’re scoring. So, for position 1 we calculate the dot product (.) of q1and k1, then q1. k2, q1. k3 and so on,…\n",
    "\n",
    "Next we apply the “scaled” factor to have more stable gradients. The softmax function can not work properly with large values, resulting in vanishing the gradients and slowing down the learning, [2]. After “softmaxing” we multiply by the Value matrix to keep the values of the words we want to focus on and minimizing or removing the values for the irrelevant words (its value in V matrix should be very small).\n",
    "\n",
    "The formula for these operations is:\n",
    "\n",
    "\n",
    "From “Attention is all you need” paper by Vaswani, et al., 2017 [1]. Scaled dot-product attention formula.\n",
    "\n",
    "Multi-head Attention\n",
    "In the previous description the attention scores are focused on the whole sentence at a time, this would produce the same results even if two sentences contain the same words in a different order. Instead, we would like to attend to different segments of the words. “We can give the self attention greater power of discrimination, by combining several self attention heads, dividing the words vectors into a fixed number (h, number of heads) of chunks, and then self-attention is applied on the corresponding chunks, using Q, K and V sub-matrices.”, [2] Peter Bloem, “Transformers from scratch”. This produce h different output matrices of scores.\n",
    "\n",
    "\n",
    "From “Attention is all you need” paper by Vaswani, et al., 2017 [1]\n",
    "But the next layer (the Feed-Forward layer) is expecting just one matrix, a vector for each word, so “after calculating the dot product of every head, we concatenate the output matrices and multiply them by an additional weights matrix Wo,”[3]. This final matrix captures information from all the attention heads.\n",
    "\n",
    "\n",
    "Positional Encoding\n",
    "We mentioned briefly that the order of the words in the sentence is an issue to solve in this model, because the network and the self-attention mechanism is permutation invariant. If we shuffle up the words in the input sentence, we get the same solutions. We need to create a representation of the position of the word in the sentence and add it to the word embedding.\n",
    "\n",
    "To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension as the embeddings, so that the two can be summed. There are many choices of positional encodings.\n",
    "\n",
    "“Attention is all you need” paper\n",
    "\n",
    "So, we apply a function to map the position in the sentence to a real valued vector. The network will learn how to use this information. Another approach would be to use a position embedding, similar to word embedding, coding every known position with a vector. “It would requiere sentences of all accepted positions during the training loop but positional encoding allow the model to extrapolate to sequence lengths longer than the ones encountered during training”, [2].\n",
    "\n",
    "In the paper a sinusoidal function is applied:\n",
    "\n",
    "\n",
    "From “Attention is all you need” paper by Vaswani, et al., 2017 [1]. Positional encoding\n",
    "\n",
    "The encoder\n",
    "Now that all the main pieces of the model have been described we can introduce the encoder components, [4]:\n",
    "\n",
    "Positional encoding: Add the position encoding to the input embedding (our input words are transformed to embedding vectors). “The same weight matrix is shared between the two embedding layers (encoder and decoder) and the pre-softmax linear transformation. In the embedding layers, we multiply those weights by square root of the model dimension” [1].\n",
    "N=6 identical layers, containing two sub-layers: a multi-head self-attention mechanism, and a fully connected feed-forward network (two linear transformations with a ReLU activation). But it is applied position-wise to the input, which means that the same neural network is applied to every single “token” vector belonging to the sentence sequence.\n",
    "\n",
    "There is a residual connection around each sub-layer (attention and FC network), summing up the output of the layer with its input, followed by a layer normalization.\n",
    "Before every residual connection, a regularization is applied: “We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks” [1] with a dropout rate of 0.1.\n",
    "Normalization and residual connections are standard tricks used to help deep neural networks train faster and more accurately. The layer normalization is applied over the embedding dimension only.\n",
    "\n",
    "Peter Bloem, “Transformers from scratch” [2]\n",
    "\n",
    "First we implement the encoder layer, each one of the six blocks, contained in an encoder:\n",
    "\n",
    "\n",
    "The next figure will show the components detailed:\n",
    "\n",
    "\n",
    "“The Ilustrated Transformer” by Jay Alammar [3]\n",
    "And the encoder code:\n",
    "\n",
    "\n",
    "Keep in mind that only the vector from the last layer (6-th) is sent to the decoder.\n",
    "\n",
    "The Decoder\n",
    "The decoder share some components with the encoder but they are used in a different way to take into account the encoder output, [4]:\n",
    "\n",
    "Positional encoding: Similar that the one in the encoder\n",
    "N=6 identical layers, containing 3 three sub-layers. First, the Masked Multi-head attention or masked causal attention to prevent positions from attending to subsequent positions. “This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i” [1]. It is implemented setting to −∞ the values corresponding to the forbidden states in the softmax layer of the dot-product attention modules. The second component or “encoder-decoder attention” performs multi-head attention over the output of the decoder, the Key and Value vectors come from the output of the encoder but the queries come from the previous decoder layer. “This allows every position in the decoder to attend over all positions in the input sequence” [1]. And finally the fully-connected network.\n",
    "The residual connection and layer normalization around each sub-layer, similar to the encoder.\n",
    "And repeat the same residual dropout that was executed in the encoder.\n",
    "The decoder layer:\n",
    "\n",
    "\n",
    "\n",
    "“The Ilustrated Transformer” by Jay Alammar [3]\n",
    "At the end of the N stacked decoders, the linear layer, a fully-connected network, transforms the stacked outputs to a much larger vector, the logits. “The softmax layer then turns those scores (logits) into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step”, [3] Jay Alammar, “The Ilustrated Transformer” .\n",
    "\n",
    "The decoder component:\n",
    "\n",
    "\n",
    "Joining all the pieces: the Transformer\n",
    "Once we have defined our components and created the encoder, the decoder and the linear-softmax final layer, we join the pieces to form our model, the Transformer.\n",
    "\n",
    "It is worth mentioning that we create 3 masks, each of which will allow us:\n",
    "\n",
    "Encoder mask: It is a padding mask to discard the pad tokens from the attention calculation.\n",
    "Decoder mask 1: this mask is a union of the padding mask and the look ahead mask which will help the causal attention to discard the tokens “in the future”. We take the maximum value between the padding mask and the look ahead one.\n",
    "Decoder mask 2: it is the padding mask and is applied in the encoder-decoder attention layer.\n",
    "\n",
    "As you can see, then we call the encoder, the decoder and the final linear-softmax layer to get the predicted output from our Transformer model.\n",
    "\n",
    "\n",
    "Picture by Gerd Altmann From Pixabay\n",
    "Training the Transformer model\n",
    "Now that we have described in detail the components in the paper, we are ready to implement them and train a transformer model on a NMT problem. It is a toy problem for educational purposes.\n",
    "\n",
    "We won’t deal with the data wrangling in this blog post. Follow the link I mentioned in the introduction for more information and the code provided to see how the data is loaded and prepared. In summary, create the vocabulary, tokenize (including a eos and sos token) and pad the sentences. Then we create a Dataset, a batch data generator, for training on batches.\n",
    "\n",
    "We need to create a custom loss function to mask the padding tokens.\n",
    "\n",
    "\n",
    "We use an Adam optimizer described in the paper, with beta1=0.9, beta2=0.98 and epsilon=10e-9 . And then we create a scheduler to vary the learning rate over the training process according to:\n",
    "\n",
    "\n",
    "“Attention is all you need” paper. Learning rate decay.\n",
    "The main train function\n",
    "The train function is similar to many other Tensorflow trainings, an usual training loop for sequence-to-sequence tasks:\n",
    "For every iteration on the batch generator that produce batch size inputs and outputs\n",
    "Get the input sequence from 0 to length-1 and the actual outputs from 1 to length, the next word expected at every sequence step.\n",
    "Call the transformer to get the predictions\n",
    "Calculate the loss function between the real outputs and the predictions\n",
    "Apply the gradients to update the weights in the model and update the optimizer too\n",
    "Calculate the mean loss and the accuracy for the batch data\n",
    "Show some results and save the model in every epoch\n",
    "And that’s all, we have all the necessary elements to train our model, we just need to create them and call the train function:\n",
    "Photo by Jr Korpa on Unsplash\n",
    "Make prediction\n",
    "When training a ML model we are not only interested in optimize losses or accuracies, we want our model to make good enough predictions and, in this case, see how the model works with new sentences. The predict function will input a tokenized sentence to the model and return the predicted new sentence, in our example, a translation from English to Spanish.\n",
    "\n",
    "These are the steps in that process:\n",
    "Tokenize the input sentence to a sequence of tokens\n",
    "Set the initial output sequence to the sos token\n",
    "Until we reach the max length or the eos token is returned by the model\n",
    "Get the next word predicted. The model returns the logits, remember that the softmax function is applied in the loss calculation.\n",
    "Get the index in the vocabulary of the word with the highest probability\n",
    "Concatenate the next word predicted to the output sequence\n",
    "And finally our last function receives a sentence in English, calls the transformer to translate it to Spanish and shows the result.\n",
    "For this example, we just experiment with some values for the model dimension and the units of the feedforward network to train the model for an hour. If you want to optimize the model, you should probably train it for longer and with many different values for the hyperparameters.\n",
    "The code is available in my github repository “Transformer-NMT”. The code is partially extracted from an excellent course by SuperDataScience Team called “Modern Natural Language Processing in Python” on Udemy. I highly recommend it.\n",
    "Some examples of translations are:\n",
    "#Show some translations\n",
    "sentence = \"you should pay for it.\"\n",
    "print(\"Input sentence: {}\".format(sentence))\n",
    "predicted_sentence = translate(sentence)\n",
    "print(\"Output sentence: {}\".format(predicted_sentence))\n",
    "Input sentence: you should pay for it. \n",
    "Output sentence: Deberías pagar por ello.\n",
    "#Show some translations\n",
    "sentence = \"we have no extra money.\"\n",
    "print(\"Input sentence: {}\".format(sentence))\n",
    "predicted_sentence = translate(sentence)\n",
    "print(\"Output sentence: {}\".format(predicted_sentence))\n",
    "Input sentence: we have no extra money. \n",
    "Output sentence: No tenemos dinero extra.\n",
    "#Show some translations\n",
    "sentence = \"This is a problem to deal with.\"\n",
    "print(\"Input sentence: {}\".format(sentence))\n",
    "predicted_sentence = translate(sentence)\n",
    "print(\"Output sentence: {}\".format(predicted_sentence))\n",
    "Input sentence: This is a problem to deal with. \n",
    "Output sentence: Este problema es un problema con eso.\n",
    "I hope you enjoy experimenting with the Transformer model. In future post we will deal with another NLP tasks.\n",
    "References\n",
    "[1] Vaswani, Ashish & Shazeer, Noam & Parmar, Niki & Uszkoreit, Jakob & Jones, Llion & Gomez, Aidan & Kaiser, Lukasz & Polosukhin, Illia, “Attention is all you need” , 2017.\n",
    "[2] Peter Bloem, “Transformers from scratch” blog post, 2019.\n",
    "[3] Jay Alammar, “The Ilustrated Transformer” blog post, 2018.\n",
    "[4] Lilian Weng, “Attention? Attention!!” blog post, 2018.\n",
    "[5] Ricardo Faúndez-CArrasco, “Attention is all you need’s review” blog post, 2017\n",
    "[6] Alexander Rush, “The Annotated Transformer”, 2018, Harvard NLP group.\n",
    "NLP\n",
    "TensorFlow\n",
    "Machine Learning\n",
    "Transformers\n",
    "Getting Started\n",
    "611\n",
    "8\n",
    "Eduardo Muñoz\n",
    "Towards Data Science\n",
    "Written by Eduardo Muñoz\n",
    "498 Followers\n",
    "·\n",
    "Writer for \n",
    "Towards Data Science\n",
    "A Data scientist and Machine Learning practitioner and involved in NLP tasks and advances. Experienced Project Management Lead. Learning every day.\n",
    "Follow\n",
    "More from Eduardo Muñoz and Towards Data Science\n",
    "A Guide to the Encoder-Decoder Model and the Attention Mechanism\n",
    "Eduardo Muñoz\n",
    "Eduardo Muñoz\n",
    "in\n",
    "Better Programming\n",
    "A Guide to the Encoder-Decoder Model and the Attention Mechanism\n",
    "Create and train a neural machine translation model with attention in TF2\n",
    "12 min read\n",
    "Oct 11, 2020\n",
    "248\n",
    "The Math behind Adam Optimizer\n",
    "Cristian Leo\n",
    "Cristian Leo\n",
    "in\n",
    "Towards Data Science\n",
    "The Math behind Adam Optimizer\n",
    "Why is Adam the most popular optimizer in Deep Learning? Let’s understand it by diving into its math, and recreating the algorithm.\n",
    "16 min read\n",
    "·\n",
    "Jan 30, 2024\n",
    "1.93K\n",
    "14\n",
    "Python’s Most Powerful Decorator\n",
    "Siavash Yasini\n",
    "Siavash Yasini\n",
    "in\n",
    "Towards Data Science\n",
    "Python’s Most Powerful Decorator\n",
    "And 5 ways to use it in data science and machine learning\n",
    "11 min read\n",
    "·\n",
    "Feb 2, 2024\n",
    "2.2K\n",
    "15\n",
    "Create a Tokenizer and Train a Huggingface RoBERTa model from scratch\n",
    "Eduardo Muñoz\n",
    "Eduardo Muñoz\n",
    "in\n",
    "Analytics Vidhya\n",
    "Create a Tokenizer and Train a Huggingface RoBERTa model from scratch\n",
    "Part 1: A product names generator using an Encoder Decoder Transformer\n",
    "7 min read\n",
    "·\n",
    "Aug 16, 2021\n",
    "132\n",
    "2\n",
    "See all from Eduardo Muñoz\n",
    "See all from Towards Data Science\n",
    "Recommended from Medium\n",
    "Illustration of the overall transformer architecture from “Attention is all you need”.\n",
    "Stefan\n",
    "Stefan\n",
    "Understanding Attention and Transformers\n",
    "My notes for understanding the attention mechanism and transformer architecture used by GPT-4 and other LLMs.\n",
    "7 min read\n",
    "·\n",
    "Nov 28, 2023\n",
    "52\n",
    "Visualize attention scores of LLMs with BertViz\n",
    "Gary Fan\n",
    "Gary Fan\n",
    "Visualize attention scores of LLMs with BertViz\n",
    "BertViz is an interactive tool for visualizing attention in Transformer language models such as BERT, GPT2, or T5. It can be run inside a…\n",
    "3 min read\n",
    "·\n",
    "Sep 7, 2023\n",
    "6\n",
    "2\n",
    "Lists\n",
    "Predictive Modeling w/ Python\n",
    "20 stories\n",
    "·\n",
    "920 saves\n",
    "Principal Component Analysis for ML\n",
    "Time Series Analysis\n",
    "deep learning cheatsheet for beginner\n",
    "Practical Guides to Machine Learning\n",
    "10 stories\n",
    "·\n",
    "1082 saves\n",
    "Natural Language Processing\n",
    "1215 stories\n",
    "·\n",
    "689 saves\n",
    "Image by vectorjuice on FreePik\n",
    "The New Chatbots: ChatGPT, Bard, and Beyond\n",
    "12 stories\n",
    "·\n",
    "307 saves\n",
    "Demystifying efficient self-attention\n",
    "Thomas van Dongen\n",
    "Thomas van Dongen\n",
    "i\n",
    "Towards Data Science\n",
    "\n",
    "Demystifying efficient self-attention\n",
    "A practical overview\n",
    "20 min read\n",
    "·\n",
    "Nov 7, 2022\n",
    "623\n",
    "2\n",
    "Intuition for Multi-headed Attention.\n",
    "Ngieng Kianyew\n",
    "Ngieng Kianyew\n",
    "\n",
    "Intuition for Multi-headed Attention.\n",
    "Follow up on the intuition on attention mechanism\n",
    "7 min read\n",
    "·\n",
    "Sep 1, 2023\n",
    "4\n",
    "Transformer Architecture explained\n",
    "Amanatullah\n",
    "Amanatullah\n",
    "Transformer Architecture explained\n",
    "Transformers are a new development in machine learning that have been making a lot of noise lately. They are incredibly good at keeping…\n",
    "10 min read\n",
    "·\n",
    "Sep 1, 2023\n",
    "360\n",
    "2\n",
    "RNN vs. LSTM vs. Transformers: Unraveling the Secrets of Sequential Data Processing\n",
    "Rokon\n",
    "Rokon\n",
    "\n",
    "RNN vs. LSTM vs. Transformers: Unraveling the Secrets of Sequential Data Processing\n",
    "In the realm of deep learning, sequential data processing is at the heart of many applications, including natural language understanding…\n",
    "3 min read\n",
    "·\n",
    "Sep 25, 2023\n",
    "7\n",
    "See more recommendations\"\"\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiuP-jzlOhvk",
    "outputId": "6ac97f65-0d5e-4e8a-b150-b7fbc756cc87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (32, 116) into a new token 257\n",
      "merging (101, 110) into a new token 258\n",
      "merging (104, 256) into a new token 259\n",
      "merging (105, 110) into a new token 260\n",
      "merging (32, 97) into a new token 261\n",
      "merging (111, 110) into a new token 262\n",
      "merging (101, 114) into a new token 263\n",
      "merging (257, 259) into a new token 264\n",
      "merging (111, 114) into a new token 265\n",
      "merging (116, 105) into a new token 266\n",
      "merging (115, 32) into a new token 267\n",
      "merging (116, 32) into a new token 268\n",
      "merging (266, 262) into a new token 269\n",
      "merging (100, 32) into a new token 270\n",
      "merging (97, 116) into a new token 271\n",
      "merging (97, 110) into a new token 272\n",
      "merging (111, 100) into a new token 273\n",
      "merging (44, 32) into a new token 274\n",
      "merging (114, 101) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# ---\n",
    "vocab_size = 276 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"merging {pair} into a new token {idx}\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JawSOPc2Ohvk",
    "outputId": "64f247de-c3c4-4ed8-b3af-685eecbd61ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 21976\n",
      "ids length: 16862\n",
      "compression ratio: 1.30X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdP1ZmqBOhvl"
   },
   "source": [
    "### Decoding\n",
    "Given a sequence of integers in the range [0, vocab_size], what is the text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OObizjaFOhvl",
    "outputId": "d154e9ca-fd94-4bb9-f9c2-58d4d0f2084d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text\n",
    "\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxyC7vwCOhvm"
   },
   "source": [
    "### encoding\n",
    "\n",
    "The other way around: Given a string, what are the tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47HkCC0gOhvm",
    "outputId": "79e66d61-40e9-43f2-b23d-33a051baf10d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (32, 116): 257,\n",
       " (101, 110): 258,\n",
       " (104, 256): 259,\n",
       " (105, 110): 260,\n",
       " (32, 97): 261,\n",
       " (111, 110): 262,\n",
       " (101, 114): 263,\n",
       " (257, 259): 264,\n",
       " (111, 114): 265,\n",
       " (116, 105): 266,\n",
       " (115, 32): 267,\n",
       " (116, 32): 268,\n",
       " (266, 262): 269,\n",
       " (100, 32): 270,\n",
       " (97, 116): 271,\n",
       " (97, 110): 272,\n",
       " (111, 100): 273,\n",
       " (44, 32): 274,\n",
       " (114, 101): 275}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YfOyfLVYOhvm",
    "outputId": "2952733e-273f-4015-e4fc-d5ec134b72f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "  # given a string, return list of integers (the tokens)\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2:\n",
    "    stats = get_stats(tokens)\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "print(encode(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYCJnLu9Ohvm",
    "outputId": "fa20ead6-85f2-43d0-8060-85337b71352a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cc3EGcBOhvn",
    "outputId": "b4771d12-9c1d-4752-dea2-51a08c8ec991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZU2Qwf-5Ohvn",
    "outputId": "701bf552-afd9-474c-8715-b038ed8b2827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "valtext = \"Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters.\"\n",
    "valtext2 = decode(encode(valtext))\n",
    "print(valtext2 == valtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FT3vpeELOhvn"
   },
   "source": [
    "### Forced splits using regex patterns (GPT series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJRbaD7gOhvn",
    "outputId": "0ad97243-7053-4203-e1ff-0984e791ce2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', '     ', ' Hello', \"'ve\", ' world', '123', ' how', \"'s\", ' are', ' you', '!!!?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "gpt4pat = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "print(re.findall(gpt2pat, \"M      Hello've world123 how's are you!!!?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzf3pOJmOhvo",
    "outputId": "ade58ffa-605d-452e-8fb5-b58e43bf197c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' ', '101', '):\\n', '   ', ' if', ' i', ' %', ' ', '3', ' ==', ' ', '0', ' and', ' i', ' %', ' ', '5', ' ==', ' ', '0', ':\\n', '       ', ' print', '(\"', 'FizzBuzz', '\")\\n', '   ', ' elif', ' i', ' %', ' ', '3', ' ==', ' ', '0', ':\\n', '       ', ' print', '(\"', 'Fizz', '\")\\n', '   ', ' elif', ' i', ' %', ' ', '5', ' ==', ' ', '0', ':\\n', '       ', ' print', '(\"', 'Buzz', '\")\\n', '   ', ' else', ':\\n', '       ', ' print', '(i', ')\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "print(re.findall(gpt4pat, example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDgBBN3mOhvo",
    "outputId": "320c1def-8c94-4a17-d33e-96dce1cabcdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDWEDqqjOhvo"
   },
   "source": [
    "Reference the GPT-2 [encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "Download the vocab.bpe and encoder.json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioAE7csOPLRH",
    "outputId": "0ce87c07-38e5-47ca-bf44-9c62edeb8401"
   },
   "outputs": [],
   "source": [
    "#!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "#!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "4OlIfgdkOhvo"
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\"\n",
    "\n",
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "# ^---- ~equivalent to our \"merges\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUQfhW8kOhvp"
   },
   "source": [
    "### special tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJ0NqlCMOhvp",
    "outputId": "2626a204-7b77-44f7-829f-2564967359e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder) # 256 raw byte tokens. 50,000 merges. +1 special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifmdEswXOhvp",
    "outputId": "df3912ed-8a3b-4ff5-8c36-f739fcbc55ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['<|endoftext|>'] # the only special token in use for the GPT-2 base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WaljmRJ3Ohvp",
    "outputId": "da7b919e-0c48-4e4b-8195-830b4f51878e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31495, 230, 75265, 243, 92245, 62904, 233, 320, 15339, 304, 16526, 16715]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\") # GPT-4 tokenizer\n",
    "print(enc.encode(\"안녕하세요 👋 (hello in Korean!)\"))\n",
    "print(enc.decode(enc.encode(\"안녕하세요 👋 (hello in Korean!)\")) == \"안녕하세요 👋 (hello in Korean!)\")\n",
    "# match the above for your own tokenizer, and also implement a train() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuLttnDcOhvq"
   },
   "source": [
    "### sentencepiece\n",
    "\n",
    "Commonly used because (unlike tiktoken) it can efficiently both train and inference BPE tokenizers. It is used in both Llama and Mistral series.\n",
    "\n",
    "[sentencepiece on Github link](https://github.com/google/sentencepiece).\n",
    "\n",
    "**The big difference**: sentencepiece runs BPE on the Unicode code points directly! It then has an option `character_coverage` for what to do with very very rare codepoints that appear very few times, and it either maps them onto an UNK token, or if `byte_fallback` is turned on, it encodes them with utf-8 and then encodes the raw bytes instead.\n",
    "\n",
    "TLDR:\n",
    "\n",
    "- tiktoken encodes to utf-8 and then BPEs bytes\n",
    "- sentencepiece BPEs the code points and optionally falls back to utf-8 bytes for rare code points (rarity is determined by character_coverage hyperparameter), which then get translated to byte tokens.\n",
    "\n",
    "(Personally I think the tiktoken way is a lot cleaner...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "nh9-VZE7Ohvq"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "L-2m9eDeOhvq"
   },
   "outputs": [],
   "source": [
    "# write a toy.txt file with some random text\n",
    "with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "  f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ficMmQ5VOhvq"
   },
   "source": [
    "Docs for sentencepiece options:\n",
    "\n",
    "- [markdown](https://github.com/google/sentencepiece/blob/master/doc/options.md)\n",
    "- [protobuf](https://github.com/google/sentencepiece/blob/master/src/sentencepiece_model.proto#L193)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSv1vfIVOhvr"
   },
   "outputs": [],
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"toy.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=4192, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMza4nOgOhvr",
    "outputId": "c7aa1eac-67f7-4e6f-f816-ef3d4f112fcd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['en', 259],\n",
       " ['▁t', 260],\n",
       " ['ce', 261],\n",
       " ['in', 262],\n",
       " ['ra', 263],\n",
       " ['▁a', 264],\n",
       " ['de', 265],\n",
       " ['er', 266],\n",
       " ['▁s', 267],\n",
       " ['ent', 268],\n",
       " ['or', 269],\n",
       " ['pr', 270],\n",
       " ['▁m', 271],\n",
       " ['▁u', 272],\n",
       " ['ing', 273],\n",
       " ['▁th', 274],\n",
       " ['ence', 275],\n",
       " ['entence', 276],\n",
       " ['Pi', 277],\n",
       " ['ed', 278],\n",
       " ['em', 279],\n",
       " ['ex', 280],\n",
       " ['is', 281],\n",
       " ['iz', 282],\n",
       " ['la', 283],\n",
       " ['on', 284],\n",
       " ['st', 285],\n",
       " ['▁S', 286],\n",
       " ['Pie', 287],\n",
       " ['end', 288],\n",
       " ['ext', 289],\n",
       " ['▁an', 290],\n",
       " ['▁pr', 291],\n",
       " ['▁to', 292],\n",
       " ['▁un', 293],\n",
       " ['▁the', 294],\n",
       " ['Piece', 295],\n",
       " ['▁Sentence', 296],\n",
       " ['▁SentencePiece', 297],\n",
       " ['.]', 298],\n",
       " ['Ne', 299],\n",
       " ['ag', 300],\n",
       " ['do', 301],\n",
       " ['ec', 302],\n",
       " ['gu', 303],\n",
       " ['ic', 304],\n",
       " ['ir', 305],\n",
       " ['it', 306],\n",
       " ['ly', 307],\n",
       " ['to', 308],\n",
       " ['▁(', 309],\n",
       " ['▁[', 310],\n",
       " ['▁f', 311],\n",
       " ['▁n', 312],\n",
       " ['▁w', 313],\n",
       " ['.])', 314],\n",
       " ['age', 315],\n",
       " ['del', 316],\n",
       " ['ion', 317],\n",
       " ['ken', 318],\n",
       " ['lan', 319],\n",
       " ['ral', 320],\n",
       " ['wor', 321],\n",
       " ['yst', 322],\n",
       " ['▁Ne', 323],\n",
       " ['▁al', 324],\n",
       " ['▁de', 325],\n",
       " ['▁is', 326],\n",
       " ['▁ma', 327],\n",
       " ['▁mo', 328],\n",
       " ['izer', 329],\n",
       " ['rain', 330],\n",
       " ['ural', 331],\n",
       " ['▁and', 332],\n",
       " ['▁lan', 333],\n",
       " ['▁pre', 334],\n",
       " ['guage', 335],\n",
       " ['ystem', 336],\n",
       " ['▁text', 337],\n",
       " ['▁model', 338],\n",
       " ['▁train', 339],\n",
       " ['kenizer', 340],\n",
       " ['▁system', 341],\n",
       " ['▁language', 342],\n",
       " ['▁training', 343],\n",
       " ['.,', 344],\n",
       " ['BP', 345],\n",
       " ['Ku', 346],\n",
       " ['ab', 347],\n",
       " ['as', 348],\n",
       " ['at', 349],\n",
       " ['by', 350],\n",
       " ['co', 351],\n",
       " ['es', 352],\n",
       " ['et', 353],\n",
       " ['if', 354],\n",
       " ['ig', 355],\n",
       " ['im', 356],\n",
       " ['ke', 357],\n",
       " ['lo', 358],\n",
       " ['nr', 359],\n",
       " ['oc', 360],\n",
       " ['e', 361],\n",
       " ['▁', 362],\n",
       " ['n', 363],\n",
       " ['t', 364],\n",
       " ['i', 365],\n",
       " ['r', 366],\n",
       " ['a', 367],\n",
       " ['o', 368],\n",
       " ['s', 369],\n",
       " ['d', 370],\n",
       " ['c', 371],\n",
       " ['l', 372],\n",
       " ['u', 373],\n",
       " ['g', 374],\n",
       " ['m', 375],\n",
       " ['p', 376],\n",
       " ['.', 377],\n",
       " ['h', 378],\n",
       " ['-', 379],\n",
       " ['w', 380],\n",
       " ['y', 381],\n",
       " ['P', 382],\n",
       " ['S', 383],\n",
       " ['b', 384],\n",
       " ['f', 385],\n",
       " ['k', 386],\n",
       " [')', 387],\n",
       " ['x', 388],\n",
       " ['z', 389],\n",
       " ['(', 390],\n",
       " ['N', 391],\n",
       " ['[', 392],\n",
       " [']', 393],\n",
       " ['v', 394],\n",
       " [',', 395],\n",
       " ['/', 396],\n",
       " ['B', 397],\n",
       " ['E', 398],\n",
       " ['K', 399]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxwZfb8lOhvr",
    "outputId": "9322297f-1633-4370-a9ae-0c8802537d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362, 378, 361, 372, 358, 362, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]\n"
     ]
    }
   ],
   "source": [
    "ids = sp.encode(\"hello 안녕하세요\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKaWXIaiOhvs",
    "outputId": "f353d335-d03b-4463-d87b-982fc5d2a0e7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'h', 'e', 'l', 'lo', '▁', '<0xEC>', '<0x95>', '<0x88>', '<0xEB>', '<0x85>', '<0x95>', '<0xED>', '<0x95>', '<0x98>', '<0xEC>', '<0x84>', '<0xB8>', '<0xEC>', '<0x9A>', '<0x94>']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puqosBOJOhvs"
   },
   "source": [
    "**Llama 2 tokenizer proto**\n",
    "If you'd like to export the raw protocol buffer for the `tokenizer.model` released by meta, this is a [helpful issue](https://github.com/google/sentencepiece/issues/121). And this is the result:\n",
    "\n",
    "```\n",
    "normalizer_spec {\n",
    "  name: \"identity\"\n",
    "  precompiled_charsmap: \"\"\n",
    "  add_dummy_prefix: true\n",
    "  remove_extra_whitespaces: false\n",
    "  normalization_rule_tsv: \"\"\n",
    "}\n",
    "\n",
    "trainer_spec {\n",
    "  input: \"/large_experiments/theorem/datasets/MERGED/all.test1.merged\"\n",
    "  model_prefix: \"spm_model_32k_200M_charcov099995_allowWSO__v2\"\n",
    "  model_type: BPE\n",
    "  vocab_size: 32000\n",
    "  self_test_sample_size: 0\n",
    "  input_format: \"text\"\n",
    "  character_coverage: 0.99995\n",
    "  input_sentence_size: 200000000\n",
    "  seed_sentencepiece_size: 1000000\n",
    "  shrinking_factor: 0.75\n",
    "  num_threads: 80\n",
    "  num_sub_iterations: 2\n",
    "  max_sentence_length: 4192\n",
    "  shuffle_input_sentence: true\n",
    "  max_sentencepiece_length: 16\n",
    "  split_by_unicode_script: true\n",
    "  split_by_whitespace: true\n",
    "  split_by_number: true\n",
    "  treat_whitespace_as_suffix: false\n",
    "  split_digits: true\n",
    "  allow_whitespace_only_pieces: true\n",
    "  vocabulary_output_piece_score: true\n",
    "  hard_vocab_limit: true\n",
    "  use_all_vocab: false\n",
    "  byte_fallback: true\n",
    "  required_chars: \"\"\n",
    "  unk_id: 0\n",
    "  bos_id: 1\n",
    "  eos_id: 2\n",
    "  pad_id: -1\n",
    "  unk_surface: \" \\342\\201\\207 \"\n",
    "  unk_piece: \"<unk>\"\n",
    "  bos_piece: \"<s>\"\n",
    "  eos_piece: \"</s>\"\n",
    "  pad_piece: \"<pad>\"\n",
    "  train_extremely_large_corpus: false\n",
    "  enable_differential_privacy: false\n",
    "  differential_privacy_noise_level: 0.0\n",
    "  differential_privacy_clipping_threshold: 0\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
